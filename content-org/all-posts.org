#+hugo_base_dir: ../
#+options: author:nil
* DONE Setting up rgee :rgee:R:
CLOSED: [2024-08-13 Tue 11:37]
:PROPERTIES:
:EXPORT_FILE_NAME: setting-up-rgee
:END:

**  Introduction

From the [[https://github.com/r-spatial/rgee][rgee github page]]: "rgee is an R binding package for calling Google Earth Engine API from within R. Various functions are implemented to simplify the connection with the R spatial ecosystem."

This guide assumes use of RStudio on a Mac, but I personally use Emacs with ESS and found it to work wonderfully. Any Unix based system should work similarly. Windows should theoretically work by following the below, but more work might be required and I haven't tested it.

** Prerequisites

- R and RStudio (or other IDE)
- Google Earth Engine account.
  - Create a GEE project to work in.
- A healthy mindset ðŸ˜Š

** Installation

Install rgee, geojsonio and miniconda.

#+begin_src R
install.packages('rgee')
install.packages('reticulate')
install.packages('geojsonio')
reticulate::install_miniconda()
#+end_src

** Setup

Load the packages.

#+begin_src R
library(rgee)
library(geojsonio)
library(reticulate)
#+end_src

Install dependencies and double check to make sure everything is up to date.

#+begin_src R
rgee::ee_install()
rgee::ee_install_upgrade()
#+end_src

By this point, you should be ready to initialize rgee. To double check everything is ready, run a
#+begin_src R
ee_check()
#+end_src
to make sure. /Note: If the previous command complains about an argument being of length zero, it should be fine to ignore it and carry on./


** Initializing
If you haven't already, you will need to create a new project to work in. This process is semi annoying if you haven't already done it before, full disclaimer.

Navigate over to [[https://code.earthengine.google.com][the GEE code editor]] and click on your profile picture in the top right. Click "Register a new Cloud Project". Assuming this is correct, click "Register a Noncommercial or Commercial project" and then select "Unpaid usage". Select the relevent project type in the dropdown menu.

On the next screen, choose "Create a new Google Cloud Project". You can leave the Organization field blank, but you will need to at the least choose a project ID. I usually name my projects with the *ee-username-rgee-#*, where username is my username and # is a number. Note that creating the project can take a while. Don't refresh the page, as you will be forced to go through the above process again.

Once you've confirmed your project's information, you will be redirected to the GEE online code editor, where you can close the tab.

Moving back into the IDE, we can now initialize rgee.
#+begin_src R
ee_Initialize()
#+end_src

This should launch a page in your web browser where you can log in and select the project you created to link to rgee. Make sure to choose "Select an existing project" and select it.

If that didn't work, or threw you an error, don't panic. I had enormous amounts of trouble getting rgee to initialize properly. If this is the case for you, you can instead run the below two commands, which does the same thing.

/Note that you will need to replace the project name with your own./
#+begin_src R
ee$Authenticate(auth_mode='localhost')
ee$Initialize(project='YOUR-PROJECT-NAME')
#+end_src

Optionally, check your connectivity to GEE to see if everything is setup correctly.
#+begin_src R
ee$String('Hello from the Earth Engine servers!')$getInfo()
#+end_src

If one of those methods worked, pop the champagne.

*At the start of every new R session where you want to use rgee, you will need to load and then initialize the package in order to use it.*

This should look something like this

#+begin_src R
library(rgee)
ee$Authenticate(auth_mode='localhost')
ee$Initialize(project='YOUR-PROJECT-NAME')
#+end_src

Remember to change the project name to the one you created earlier.

** Examples
*** Surface water occurrence
In this example, rgee is used to visualize global surface water occurrence using the JRC Global Surface Water dataset.

/Load and initialize rgee first!/
#+begin_src R
# Loads the dataset from JRC
gsw <- ee$Image("JRC/GSW1_1/GlobalSurfaceWater")
occurrence <- gsw$select("occurrence")

# Defines params to show surface water
VIS_OCCURRENCE <- list(
  min = 0,
  max = 100,
  palette = c("red", "blue")
)

# Makes a mask with said params
VIS_WATER_MASK <- list(
  palette = c("white", "black")
)

# Creates another mask that only shows areas w/ 90% water occurance
water_mask <- occurrence$gt(90)$selfMask()

# Sets the center of the map to Douglas County
Map$setCenter(-95.3, 38.91, 11)

# Adds both masks to the map
Map$addLayer(occurrence$updateMask(occurrence$divide(100)), VIS_OCCURRENCE, "Water Occurrence (1984-2018)") +
Map$addLayer(water_mask, VIS_WATER_MASK, "90% occurrence", FALSE)
{% endhighlight %}

#+end_src
*** May 28th 2019 Tornado

In this example, rgee is used to view the damage path from the 2019 EF4 tornado that touched down in central Douglas County.

/Load and initialize rgee first!/
#+begin_src R

# Defines an area around Douglas County.
aoi <- ee$Geometry$Rectangle(c(-95.3, 38.85, -95.22, 38.91))

# Defines the dates we are interested in.
start_date <- '2019-05-28'
end_date <- '2019-06-01'
pre_event_date <- '2019-05-20'

# Load Sentinel-2 data for said dates.
s2_collection <- ee$ImageCollection('COPERNICUS/S2')$
  filterDate(start_date, end_date)$
  filterBounds(aoi)

# Get the least cloudy image from the post-event period.
post_event_image <- s2_collection$sort('CLOUDY_PIXEL_PERCENTAGE')$first()

# Define params for said image.
viz_params <- list(
  bands = c('B4', 'B3', 'B2'),
  min = 0,
  max = 3000,
  gamma = 1.4
)

# Add the post-event image to the map.
Map$centerObject(aoi, 12)  # Zoom in to level 12 for better detail
Map$addLayer(post_even  t_image, viz_params, 'Post-Event Sentinel-2 Image')

# Load pre-event data for comparison.
pre_event_image <- ee$ImageCollection('COPERNICUS/S2')$
  filterDate(pre_event_date, start_date)$
  filterBounds(aoi)$
  sort('CLOUDY_PIXEL_PERCENTAGE')$first()


# Calculate NDVI for pre and post tornado.
pre_ndvi <- pre_event_image$normalizedDifference(c('B8', 'B4'))
post_ndvi <- post_event_image$normalizedDifference(c('B8', 'B4'))

# Find difference.
ndvi_diff <- post_ndvi$subtract(pre_ndvi)

# Define params for NDVI difference.
ndvi_viz_params <- list(min = -0.5, max = 0.5, palette = c('red', 'yellow', 'green'))

# Add NDVI diff. layer to map!
Map$addLayer(ndvi_diff, ndvi_viz_params, 'NDVI Difference')
{% endhighlight %}
#+end_src

* DONE Using variables in rgee :rgee:
CLOSED: [2024-08-13 Tue 11:37]
:PROPERTIES:
:EXPORT_FILE_NAME: using-variables-in-rgee
:END:
** Introduction

Google Earth Engine provides access to a bunch of geospatial datasets including satellite imagery, climate data and land cover classifications. These datasets, known as variables, are used to perform geospatial analyses.

** Calling variables
/Load and initialize rgee first!/

Define an area that you want to visualize. The easiest way of doing this is to define a rectangle with coordinates.

#+begin_src R
aoi <- ee$Geometry$Rectangle(c(-120.4, 34.5, -119.4, 35.5))
#+end_src

In this example, I'll use the MODIS land cover datasets.

#+begin_src R
landcover <- ee$ImageCollection("MODIS/061/MCD12Q1")$first()$select("LC_Type1")
#+end_src

Set some visualization parameters to control how the data is displayed.

#+begin_src R
landcover_viz <- list(
  min = 1,
  max = 17,
  palette = c("05450a", "086a10", "54a708", "78d203", "009900",
              "c6b044", "dcd159", "dade48", "fbff13", "b6ff05",
              "27ff87", "c24f44", "a5a5a5", "ff6d4c", "69fff8",
              "f9ffa4", "1c0dff")
)
#+end_src

Center the map on the area we are interested in, and add in the land cover layer using the parameters we defined above.

#+begin_src R
Map$centerObject(aoi,8)
Map$addLayer(landcover, landcover_viz, "Land Cover :)")
#+end_src
* DONE Making maps of variables with rgee :rgee:
CLOSED: [2024-08-13 Tue 11:37]
:PROPERTIES:
:EXPORT_FILE_NAME: Making-maps-of-variables-with-rgee
:END:
** Introduction

Raw data often needs to be transformed in order to do anything useful with it. Typically, transforming variables in GEE involves manipulating raw bands of imagery to create products like NDVI and EVI, or performing math operations with multiple datasets.

*** Dividing EVI by Precipitation
In this example, EVI is being divided by precipitation data for Kansas, and the result is mapped.

/Load and initialize rgee first!/
#+BEGIN_SRC R
library(rgee)
ee_Initialize()

# Define an area of interest (AOI) over Kansas
aoi <- ee$Geometry$Rectangle(c(-102.05, 36.99, -94.6, 40.0))

# Load the Sentinel-2 image collection and calculate EVI
s2_collection <- ee$ImageCollection("COPERNICUS/S2")$
  filterDate('2020-06-01', '2020-08-31')$
  filterBounds(aoi)$
  map(function(image) {
    evi <- image$expression(
      '2.5 * ((NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1))',
      list('NIR' = image$select("B8"), 'RED' = image$select("B4"), 'BLUE' = image$select("B2"))
    )$rename("EVI")
    return(image$addBands(evi))
  })
evi_image <- s2_collection$select("EVI")$mean()$clip(aoi)

# Visualize EVI to ensure it's calculated correctly
evi_viz <- list(
  min = -1,
  max = 1,
  palette = c("brown", "yellow", "green")
)
Map$setCenter(-98.35, 38.5, 6)
Map$addLayer(evi_image, evi_viz, "EVI")

# Load the TerraClimate PPT (precipitation) dataset
ppt_dataset <- ee$ImageCollection("IDAHO_EPSCOR/TERRACLIMATE")$
  filterDate('2020-01-01', '2020-12-31')$
  select("pr")$
  mean()$
  clip(aoi)

# Visualize PPT to ensure it's loaded correctly
ppt_viz <- list(
  min = 0,
  max = 2000,
  palette = c("blue", "white", "green")
)
Map$addLayer(ppt_dataset, ppt_viz, "Precipitation")

# Ensure the datasets align perfectly for each pixel
evi_resampled <- evi_image$reproject(crs = ppt_dataset$projection(), scale = 1000)
ppt_resampled <- ppt_dataset$reproject(crs = evi_image$projection(), scale = 1000)

# Divide EVI by PPT
evi_ppt_ratio <- evi_resampled$divide(ppt_resampled)

# Define visualization parameters for the ratio
evi_ppt_viz <- list(
  min = 0,
  max = 0.1,
  palette = c("blue", "white", "red")
)

# Add the EVI/PPT ratio layer
Map$addLayer(evi_ppt_ratio, evi_ppt_viz, "EVI/PPT Ratio")

#+END_SRC

*** Breakdown

**** Define the AOI, the state of Kansas.

#+BEGIN_SRC R
aoi <- ee$Geometry$Rectangle(c(-102.05, 36.99, -94.6, 40.0))
#+END_SRC

**** Load and Filter Sentinel-2 Data

Next, we load the Sentinel-2 image collection, filter it for the summer months (June to August 2020), and calculate EVI for each image. EVI values are then averaged and clipped only to our AOI.

#+BEGIN_SRC R
s2_collection <- ee$ImageCollection("COPERNICUS/S2")$
  filterDate('2020-06-01', '2020-08-31')$
  filterBounds(aoi)$
  map(function(image) {
    evi <- image$expression(
      '2.5 * ((NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1))',
      list('NIR' = image$select("B8"), 'RED' = image$select("B4"), 'BLUE' = image$select("B2"))
    )$rename("EVI")
    return(image$addBands(evi))
  })
evi_image <- s2_collection$select("EVI")$mean()$clip(aoi)
#+END_SRC

**** First mapping of EVI

This is to make sure the current values are correct and being visualized correctly. I had a lot of trouble with this example in particular, so I am redundantly checking work as we go.

#+BEGIN_SRC R
evi_viz <- list(
  min = -1,
  max = 1,
  palette = c("brown", "yellow", "green")
)
Map$setCenter(-98.35, 38.5, 6)
Map$addLayer(evi_image, evi_viz, "EVI")
#+END_SRC

**** Loading precip data

We load precip data from TerraClimate, filter it for the year 2020, average values, and clip them.

#+BEGIN_SRC R
ppt_dataset <- ee$ImageCollection("IDAHO_EPSCOR/TERRACLIMATE")$
  filterDate('2020-01-01', '2020-12-31')$
  select("pr")$
  mean()$
  clip(aoi)
#+END_SRC

**** Visualize PPT to make sure it's correct

Again, I am visualizing the precipitation data to make sure it looks correct.

#+BEGIN_SRC R
ppt_viz <- list(
  min = 0,
  max = 2000,
  palette = c("blue", "white", "green")
)
Map$addLayer(ppt_dataset, ppt_viz, "Precipitation")
#+END_SRC

**** Make sure data is aligned

Before actually performing any operations on the data, making sure that both datasets are aligned correctly is important. Here I'm reprojecting both sets to the same scale and resolution.

#+BEGIN_SRC R
evi_resampled <- evi_image$reproject(crs = ppt_dataset$projection(), scale = 1000)
ppt_resampled <- ppt_dataset$reproject(crs = evi_image$projection(), scale = 1000)
#+END_SRC
**** Transform the data

Dividing the EVI by the precip to calculate the ratio.

#+BEGIN_SRC R
evi_ppt_ratio <- evi_resampled$divide(ppt_resampled)
#+END_SRC

**** Define viz params for mapping the new ratio


#+BEGIN_SRC R
evi_ppt_viz <- list(
  min = 0,
  max = 0.1,
  palette = c("blue", "white", "red")
)
Map$addLayer(evi_ppt_ratio, evi_ppt_viz, "EVI/PPT Ratio")
#+END_SRC
* DONE Exporting data to a CSV with rgee :rgee:
CLOSED: [2024-08-13 Tue 11:37]
:PROPERTIES:
:EXPORT_FILE_NAME: Exporting-data-to-a-csv-with-rgee
:END:
** Introduction

In general, exporting data to a .csv with rgee is pretty easy. The general gist of the process is:
- Define the area or points you are interested in
- Filter data
- Convert the results to a list, and
- Export to a csv.

** Example
/Make sure to load and initialize rgee first!/
#+BEGIN_SRC R
# Define an AOI over Kansas
aoi <- ee$Geometry$Rectangle(c(-102.05, 36.99, -94.6, 40.0))

# Define sample points in the area
points <- ee$FeatureCollection(c(
  ee$Feature(ee$Geometry$Point(-98.5795, 39.8283), list(label = "1")),
  ee$Feature(ee$Geometry$Point(-97.5795, 38.8283), list(label = "2")),
  ee$Feature(ee$Geometry$Point(-96.5795, 37.8283), list(label = "3"))
))

# Grab an image from Sentinal 2 and calculate NDVI
s2_collection <- ee$ImageCollection("COPERNICUS/S2")$
  filterDate('2020-06-01', '2020-06-30')$
  filterBounds(aoi)$
  map(function(image) {
    ndvi <- image$normalizedDifference(c("B8", "B4"))$rename("NDVI")
    return(image$addBands(ndvi))
  })
ndvi_image <- s2_collection$select("NDVI")$mean()$clip(aoi)

# Grab NDVI values at the sample points
ndvi_values <- ndvi_image$reduceRegions(
  collection = points,
  reducer = ee$Reducer$mean(),
  scale = 30
)

# Convert the result to a list and then to a data frame
ndvi_list <- ndvi_values$getInfo()$features
ndvi_df <- do.call(rbind, lapply(ndvi_list, function(x) data.frame(
  label = x$properties$label,
  NDVI = x$properties$mean,
  lon = x$geometry$coordinates[1],
  lat = x$geometry$coordinates[2]
)))

# Save the data frame as a CSV
write.csv(ndvi_df, "ndvi_values_hello.csv", row.names = FALSE)
#+END_SRC

* DONE Getting data from AppEEARS and NOAA :rgee:
CLOSED: [2024-08-13 Tue 11:37]
:PROPERTIES:
:EXPORT_FILE_NAME: Getting-data-from-AppEEARS-and-NOAA
:END:


** Data types (link to method of gathering)
- [[MAT][MAT (Mean Annual Temperature)]]
  - Average yearly temperature.
- [[MAP][MAP (Mean Annual Precipitation)]]
  - Average yearly precipitation.
- [[GPP][GPP (Gross Primary Productivity)]]
  - Total amount of energy captured by plants. Does not account for respiration losses.
- [[NPP][NPP (Net Primary Productivity)]]
  - Amount of energy that remains after plants have used some of the captured energy for their own respiration. Actual amount of new biomass that is available for consumption by other critters. NPP = GPP - Respiration
- [[PET][PET (Potential Evapotranspiration)]]
  - Amount of water that would be evaporated and transpired by vegetation if there was sufficient water available. Atmospheric demand for water.
- [[AET, ET][AET, ET (Actual Evapotranspiration)]]  - Actual amount of water that is evaporated from soil and transpired by vegetation. Less than or equal to PET. Depends on availability of water.
- [[DI][DI (Dryness Index)]]
  - PET / MAP
- [[EP][EP (Evaporation Potential)]]
  - 1 - (PET /MAP)
** Sites
This data was gathered from many sites across the globe. Sites were sorted with a RegionName, SiteName, and Pit.

| RegionName     | SiteName        | Pit          |
|----------------+-----------------+--------------|
| Calhoun        | R7              | R7P2         |
| Calhoun        | R2              | R2P1         |
| Calhoun        | R7              | R7P1         |
| Calhoun        | R8              | R8P1         |
| Calhoun        | R8              | R8P2.5       |
| Calhoun        | R8              | R8P2         |
| Calhoun        | R1              | R1C2         |
| Calhoun        | R1              | R1C3         |
| Calhoun        | R2              | R2H1         |
| Calhoun        | R7              | R7H1         |
| Calhoun        | R7              | R7H2         |
| Calhoun        | R8              | R8H1         |
| Calhoun        | R8              | R8H2.5       |
| Calhoun        | R8              | R8H2         |
| Luquillo       | ElVerde         | ElVerdeM     |
| Luquillo       | ElVerde         | ElVerdeR     |
| Luquillo       | ElVerde         | ElVerdeT     |
| Luquillo       | Icacos          | IcacosM      |
| Luquillo       | Icacos          | IcacosR      |
| Luquillo       | Icacos          | IcacosT      |
| Catalina       | MixedCon        | MC_M         |
| Catalina       | MixedCon        | MC_R         |
| Catalina       | MixedCon        | MC_T         |
| Catalina       | BigelowDesert   | B2D_M        |
| Catalina       | BigelowDesert   | B2D_R        |
| Catalina       | BigelowDesert   | B2D_T        |
| ReynoldsCr     | NorthBasalt     | NB_R         |
| ReynoldsCr     | NorthBasalt     | NB_T         |
| ReynoldsCr     | NorthLoess      | NL_T         |
| ReynoldsCr     | SWBasalt        | SWB_M        |
| ReynoldsCr     | SWBasalt        | SWB_T        |
| ReynoldsCr     | SWLoess         | SWL_T        |
| SouthernSierra | SJER            | SJER_M       |
| SouthernSierra | SJER            | SJER_R       |
| SouthernSierra | SJER            | SJER_T       |
| DukeFarm       | DukeFarm        | DFPasture    |
| EKS            | Ottawa          | EKSAgri      |
| EKS            | Welda           | EKSNative    |
| EKS            | Welda           | EKSPostAg    |
| KNZ            | KNZ             | KNZNative    |
| KNZ            | KNZ             | KNZAgri      |
| KNZ            | KNZ             | KNZPostAg    |
| HAY            | HAY             | HAYNative    |
| HAY            | HAY             | HAYAgri      |
| HAY            | HAY             | HAYPostAg    |
| TRB            | TRB             | TRBNative    |
| TRB            | TRB             | TRBAgri      |
| TRB            | TRB             | TRBAgriIrrig |
| TRB            | TRB             | TRBPostAg    |
| FRESCC         | CC1             | CC1_2020     |
| FRESCC         | CC2             | CC2_2020     |
| FRESCC         | CC2             | CC2_2022     |
| FRESCC         | CC3             | CC3_2021     |
| FRESCC         | CC3             | CC3_2022     |
| FRESCC         | CC4             | CC4_2021     |
| FRESCC         | CC5             | CC5_2021     |
| Konza          | GrassyToe       | GrToeN01B    |
| Konza          | WoodyToe        | WdToeN04D    |
| Konza          | GrassyBackslope | GrBackslN01B |
| Konza          | WoodyBackslope  | WdBackslN04D |
| Konza          | GrassySummit    | GrSummN01B   |
| Konza          | WoodySummit     | WdSummN04D   |
| HJAndrews      | WS01            | NF_Y_A       |
| HJAndrews      | WS01            | SF_Y_A       |
| HJAndrews      | WS02            | NF_O_A       |
| HJAndrews      | WS02            | SF_O_A       |
| HJAndrews      | WS03            | NF_O_A       |
| HJAndrews      | WS03            | NF_Y_A       |
| HJAndrews      | WS03            | SF_O_A       |
| HJAndrews      | WS03            | SF_Y_A       |
| Alps           | Glacier         | Alps1        |
| Alps           | GlacierRidge    | Alps2        |
| Alps           | Limestone       | Alps3        |
| Alps           | Gneiss          | Alps5        |
| Alps           | Alluvial        | Alps6        |
| NH             | ThompsonPasture | NH_TP        |
| NH             | ThompsonForest  | NH_TF        |
| NH             | OrganicPasture  | NH_OP        |
| NH             | OrganicForest   | NH_OF        |


** Process
*** MAT
Gathered mainly from [[https://www.ncei.noaa.gov/maps/annual/][NCEI at NOAA]].
- Turn on the "Annual Normals (2006-2020)" map layer, and disable the "Global Summary of the Year" layer.
- Put coords into floating search box, and find a station near enough to the site to be relevant.
- Click the wrench on "Annual Normals (2006-2020)", and use the rectangle tool to make a box around the station, just enough to select it.
- Select the station in the menu that appears on the left side of the page, and add to cart.
- In the new tab, click "Show List" under the "Data Types" text field.
- Type, filter, and select both "Annual average temperature mean" and "Annual precipitation totals"
- In the downloaded .csv, precip is labeled as ANN-PRCP-NORMAL, and temperature is labeled as ANN-TAVG-NORMAL.
*** MAP
Gathered mainly from [[https://www.ncei.noaa.gov/maps/annual/][NCEI at NOAA]].
- Turn on the "Annual Normals (2006-2020)" map layer, and disable the "Global Summary of the Year" layer.
- Put coords into floating search box, and find a station near enough to the site to be relevant.
- Click the wrench on "Annual Normals (2006-2020)", and use the rectangle tool to make a box around the station, just enough to select it.
- Select the station in the menu that appears on the left side of the page, and add to cart.
- In the new tab, click "Show List" under the "Data Types" text field.
- Type, filter, and select both "Annual average temperature mean" and "Annual precipitation totals"
- In the downloaded .csv, precip is labeled as ANN-PRCP-NORMAL, and temperature is labeled as ANN-TAVG-NORMAL.
*** NPP
NPP data is gathered from [[https://appeears.earthdatacloud.nasa.gov/][AppEEARS]].
- Navigate to Extract > Point, and make an account if you haven't.
- Start a new request.
- Name the request, and put your coordinates in the text box on the right. Comma seperated, the text should look like /ID, Category, Lat, Long/.
  - You can alternatively upload a .csv with the same formatting, useful for large pulls.
- Set the dates. For this process, I was pulling from Jan 1, 2006 - Dec 31, 2021.
- Scroll down to select layers to include in the sample.
- NPP is the first result. Make sure the data is yearly, not 8-day.
- Add other products if you wish, and submit the request.
- Depending on the size of the data requested, it can take up to a couple hours to process.
*** PET
PET data is gathered from [[https://appeears.earthdatacloud.nasa.gov/][AppEEARS]].
- Navigate to Extract > Point, and make an account if you haven't.
- Start a new request.
- Name the request, and put your coordinates in the text box on the right. Comma seperated, the text should look like /ID, Category, Lat, Long/.
  - You can alternatively upload a .csv with the same formatting, useful for large pulls.
- Set the dates. For this process, I was pulling from Jan 1, 2006 - Dec 31, 2021.
- Scroll down to select layers to include in the sample.
- Search for "Evapo yearly" select the first option.
- PET is labeled "PET", and AET is labeled "ET".
- Depending on the size of the data requested, it can take up to a couple hours to process.
*** AET, ET
AET data is gathered from [[https://appeears.earthdatacloud.nasa.gov/][AppEEARS]].
- Navigate to Extract > Point, and make an account if you haven't.
- Start a new request.
- Name the request, and put your coordinates in the text box on the right. Comma seperated, the text should look like /ID, Category, Lat, Long/.
  - You can alternatively upload a .csv with the same formatting, useful for large pulls.
- Set the dates. For this process, I was pulling from Jan 1, 2006 - Dec 31, 2021.
- Scroll down to select layers to include in the sample.
- Search for "Evapo yearly" select the first option.
- PET is labeled "PET", and AET is labeled "ET".
- Depending on the size of the data requested, it can take up to a couple hours to process.
*** DI
- DI and EP are calculated with PET and MAP products.
- PET / MAP
*** EP
- EP and DI are calculated with PET and MAP products.
- 1 - (PET / MAP)
*** GPP
GPP data is gathered from [[https://appeears.earthdatacloud.nasa.gov/][AppEEARS]].
- Navigate to Extract > Point, and make an account if you haven't.
- Start a new request.
- Name the request, and put your coordinates in the text box on the right. Comma seperated, the text should look like /ID, Category, Lat, Long/.
  - You can alternatively upload a .csv with the same formatting, useful for large pulls.
- Set the dates. For this project, I was pulling from Jan 1, 2006 - Dec 31, 2021.
- Scroll down to select layers to include in the sample.
- GPP is only available in 8-day, which creates an extremely obnoxious problem - I solved this problem with an R script

- If you need to calculate yearly GPP for many different sites, the first script is intended for that purpose.
- If you only need to calculate yearly GPP for one single site, then scroll down for instructions.

** Initial .csv setup
- Before you begin, we will need to edit the excel file that AppEEARS gives us.
  - Keep only the needed columns: ID, Category, Latitude, Longitude, Date and GPP - Note that the actual GPP column is labeled something like "MOD17A2HGF_061_Gpp_500m"
    - There are many extra columns that you will need to delete.
    - Rename the GPP column to "GPP"
- Create a new column called "Year". The goal is to have this column show the year in YYYY format for every sample.
  - In the first cell (Should be E2 or F2), enter "=YEAR(D2)", where D2 refers to your date column. Change the cell type to "General" - you should see a year in YYYY format.
  - Extend this formula down to fill in the whole Year column.
  - Copy the whole column, and then Edit > Paste Special..., and select Values. This replaces the formula dependent cells with ones that show the year in plain text.
- The list of columns should now be ID, Category, Latitude, Longitude, Date, Year, GPP.
- In the script, "appears.csv" refers to the downloaded .csv - you will need to either rename your file or change this to change the call in the file to point to the correct location.

***** Multiple sites

#+begin_src R
  library(dplyr)

  # sets initial df as the sanitzed, edited .csv
  df <- read.csv("appeears.csv", header = TRUE)

  # sets output to not be in scientific notation
  options(scipen = 999)

  # checking the df to make sure all good.
  # I had to mess around in excel a bit to get rid of empty rows at the bottom.
  head(df)
  tail(df)

  # set object to sum all the 8day GPP values
  summarized_data <- aggregate(GPP ~ Category + Year, data=df, sum, na.rm = TRUE)

  # The goal was to take the sum values and keep the Site and Pit labels
  #   in the sheet.
  names(summarized_data)[names(summarized_data) == "GPP"] <- "total_GPP"

  # Gets rid of duplicates for Category and Year rows.
  unique_rows <- df[!duplicated(df[, c("Category", "Year")]), ]

  # set object to merge the sum'd GPPs and the category and year
  summarized_all_data <- merge(unique_rows, summarized_data, by=c("Category", "Year"))

  # export a .csv with the above
  write.csv(summarized_all_data, "annual_gpp.csv", row.names=FALSE)

  # New goal: to average each sites GPP values across all the defined years 2006-2021
  # removed stray 2005 8day values from output .csv in excel
  df2 <- read.csv("annual_gpp.csv", header = TRUE)

  #checking df2
  head(df2)

  # ALERT!! I open the annual_gpp.csv here and rename columns to be "Pit" and "Site", accordingly.
  # adds mean of total_GPP, and sets that to be average_GPP
  average_GPP <- aggregate(total_GPP ~ Pit, data = df2, FUN = mean)

  # takes average_GPP value and merges it with all the label columns
  # sorts by Pit
  merged_data <- merge(average_GPP, df2[c("Site", "Pit", "Year", "Latitude", "Longitude")]
                       , by = "Pit")

  # removes duplicates
  merged_data <- merged_data[!duplicated(merged_data$Pit), ]

  #  exports a csv with all of that stuff. Hip Hip hooray
  write.csv(merged_data, file = "time_averaged_GPP.csv", row.names = FALSE)
    #+end_src

***** Single site
If you only are processing GPP data from one site, then some slight modifications are needed for the script to function.

#+begin_src R
library(dplyr)

# sets initial df as the sanitized, edited data
df <- read.csv("appeears.csv", header = TRUE)

# sets output to not be in scientific notation
options(scipen = 999)

# checking the df to make sure all good
head(df)
tail(df)

# set object to sum all the 8day GPP values
summarized_data <- aggregate(GPP ~ Year, data=df, sum, na.rm = TRUE)

# The goal was to take the sum values and keep the Site and Pit labels in the sheet
names(summarized_data)[names(summarized_data) == "GPP"] <- "total_GPP"

# Gets rid of duplicates for Year rows
unique_rows <- df[!duplicated(df$Year), ]

# set object to merge the summed GPPs and the year
summarized_all_data <- merge(unique_rows, summarized_data, by="Year")

# export a .csv with the above
write.csv(summarized_all_data, "annual_gpp.csv", row.names=FALSE)

# Read the exported .csv
df2 <- read.csv("annual_gpp.csv", header = TRUE)

# Checking df2
head(df2)

# Calculate the average of total_GPP for each combination of Latitude and Longitude
average_GPP <- aggregate(total_GPP ~ Latitude + Longitude, data = df2, FUN = mean)

# Rename the average GPP column
names(average_GPP)[names(average_GPP) == "total_GPP"] <- "average_GPP"

# Merge the average GPP values with the original data to keep all necessary columns
merged_data <- merge(average_GPP, df2[, c("ID", "Latitude", "Longitude", "Year")], by = c("Latitude", "Longitude"))

# Remove duplicates to ensure each Latitude-Longitude combination appears only once
merged_data <- merged_data[!duplicated(merged_data[, c("Latitude", "Longitude")]), ]

# Export a csv with the averaged GPP values
write.csv(merged_data, file = "final_averaged_GPP.csv", row.names = FALSE)
#+end_src

* DONE Mapping DI and EP with rgee :rgee:maps:
CLOSED: [2024-08-15 Thu 12:39]
:PROPERTIES:
:EXPORT_FILE_NAME: Mapping-DI-and-EP-with-rgee
:END:
[[/images/rgee_global.png]]
Note: I have a sneaking suspicion that trying this in Earth Engine proper using JS will significantly improve the results. The same goal attempted in rgee will remain available below, and a hopefully better JS version will appear above this in the coming days.
** Rgee version
*** Introduction
Goal: create global maps showing Dryness Index and Evaporative Index in both 2012 and 2019, using rgee.

This was pretty tricky to get working. The primary challenge was visualizing the data in a way that was meaningful, and preventing extreme values from obfuscating the scale.

**** Scale values / min and max
A major pain to get right. The values in the script below /are still not correct/ and the product generated *should not* be used for any purpose other than general visualization. Google Earth Engine documentation [[https://developers.google.com/earth-engine/datasets/catalog/IDAHO_EPSCOR_TERRACLIMATE#bands][provides estimated min and max values and scale]], but using those results in seemingly useless visualizations. After trials, I found that the best visualization came from using max values of 4 for DI layers, and 2 for EI layers. Additionally, make sure to multiple all bands by 0.1 to account for scale.

**** Color palette
Another pinch point. At first, the map being dominated by extreme values seems like an issue that could be easily solved by using a more expansive color palette. In practice, this doesn't work. Going from only three colors to over twenty just creates more range in the areas of the map where range actually exists, instead of creating it in the areas that are solidly either the min or the max.
**** Data statistics
Interesting findings here that mostly reveal my own incompetence. Doing some light sniffing on the visualized values gives some very bizarre results: for both bands, the 50th percentile is 0. For EI, the 95th percentile is 0.662 and the 99th is 0.759. Similar values are present among both years and both DI and EI.
*** Script
#+begin_src R

# Load required libraries
library(rgee)
library(ggplot2)

# Initialize rgee: see docs.mor-gan.com/posts/setting-up-rgee/#initializing
ee_check()
ee_install_upgrade()
ee$Authenticate(auth_mode='localhost')
ee$Initialize(project='ee-pugbugdude')

# Define the years of interest
years <- c(2012, 2019)

# Function to load datasets for different years - continued with snippet below.
load_dataset <- function(year) {
  ee$ImageCollection("IDAHO_EPSCOR/TERRACLIMATE")$
    filter(ee$Filter$calendarRange(year, year, "year"))$
    mean()
}

# Loads the dataset above for both defined years.
dataset_2012 <- load_dataset(2012)
dataset_2019 <- load_dataset(2019)

# Function to select bands and scale properly
# see developers.google.com/earth-engine/datasets/catalog/IDAHO_EPSCOR_TERRACLIMATE#bands
# for scaling info.
get_scaled_bands <- function(dataset) {
  list(
    pet = dataset$select('pet')$multiply(0.1),
    aet = dataset$select('aet')$multiply(0.1),
    pr = dataset$select('pr')
  )
}

# Sets the bands for both years
bands_2012 <- get_scaled_bands(dataset_2012)
bands_2019 <- get_scaled_bands(dataset_2019)

# Calculate indices
# DI (Dryness Index) = PET / PR
# EI (Evaporative Index) = AET / PR
calculate_indices <- function(bands) {
  list(
    dryness_index = bands$pet$divide(bands$pr)$rename("Dryness_Index"),
    evap_index = bands$aet$divide(bands$pr)$rename("Evaporative_Index")
  )
}

# Calculates indices for both years
indices_2012 <- calculate_indices(bands_2012)
indices_2019 <- calculate_indices(bands_2019)

# Sets the palette to use for mapping.
index_palette <- c("blue", "cyan", "green", "yellow", "orange", "red", "darkred")

# Function to put layers on the map for both years and bands
# Uses variables set above
# Why: It's easier to be able to edit the palette, min, and max values
#  for all layers at once, rather than having to keep track of several different lines.
visualize_layer <- function(image, title, min, max) {
  Map$addLayer(
    image,
    list(min = min, max = max, palette = index_palette),
    title
  )
}

# Set min and max values for visualization.
# IMPORTANT: These are very, very finicky to figure out. The min and max listed at
# developers.google.com/earth-engine/datasets/catalog/IDAHO_EPSCOR_TERRACLIMATE#bands
# will result in a wonky, poorly visualized map.
# The numbers used below are a best attempt effort that generates a product to be used purely for
# visualization, not a particularly accurate one. I strongly encourage you to adjust them to see how the map changes.
dryness_index_min <- 0
dryness_index_max <- 4

evap_index_min <- 0
evap_index_max <- 1.5

# Adds each of the layers to the map.
# In RStudio, the map should open automatically in the viewer tab.
# In other environments, the map should open in a browser tab.
visualize_layer(indices_2012$dryness_index, "DI 2012", dryness_index_min, dryness_index_max)
visualize_layer(indices_2012$evap_index, "EI 2012", evap_index_min, evap_index_max)

# Add layers for 2019
visualize_layer(indices_2019$dryness_index, "DI 2019", dryness_index_min, dryness_index_max)
visualize_layer(indices_2019$evap_index, "EI 2019", evap_index_min, evap_index_max)
#+end_src

* DONE Creating a .kml file from a Google Earth project
CLOSED: [2024-08-15 Thu 12:39]
:PROPERTIES:
:EXPORT_FILE_NAME: Creating-a-kml-from-a-project
:END:
** Introduction
"Keyhole Markup Language"

.kml files are useful for a number of reasons, namely storing pins, locations, polygons, images, and other GIS info. One of the easiest ways to make them is with Google Earth.

You can approach this in a couple of ways. Both the [[https://earth.google.com][Earth website]] and the [[https://www.google.com/earth/about/versions/][Google Earth Pro desktop app]] are usable.

** Google Earth Web
1. Open [[https://earth.google.com][Google Earth]] in a browser.
2. Open the left hand side bar. It's a little arrow in the middle of the left edge.
3. Click the "+ New" button, select "Local KML file", and click "Create".
4. You should see a new section, "Local KML files", and your new project, "Untitled".
   - You can rename the project by clicking on the kebab menu while hovering on it, and clicking "Rename".
5. Click on the project to select it.
6. To add a location to the project, navigate to it, and then click the "Save to project" button that appears in the card.
   - Alternatively, you can use the pin / placemark tool (first on the left in the toolbar) and it will be automatically added to the selected project.
   - You can rename the pin / placemark by either doing so when you initially place it, or by using the kebab menu in the project menu.
7. Once you are done adding to your project, you can export by clicking the kebab menu on it, and then selecting "Export as KML file".

** Google Earth Pro
1. Open [[https://www.google.com/earth/about/versions/][Google Earth Pro]].
2. Create a folder under Add > Folder. This is your project, so name it accordingly.
3. Create placemarks with the placemark tool.
   - They should be automatically placed into the new folder.
   - You can drag other objects inside of the folder to add them to the project.
4. Once you are done with the project, export it by right clicking on the folder and selecting "Save place as.."
   - There are two options to save: a .kml and .kmz.
   - If you don't know what to use, select .kml.
   - .kmz files are used to compress larger projects, typically ones that include images, but have some compatability issues.
* DONE Git basics
CLOSED: [2024-08-21 Wed 11:00]
:PROPERTIES:
:EXPORT_FILE_NAME: Git-basics
:END:

** Introduction to Git and GitHub
It's easiest to think of Git (and other forms of source control) like a time machine.
 * When you make changes to your project, it's easy to see what has changed.
 * You have a perfect history of everything that's ever happened in your project, and can always revert to any point in that history.
 * Git allows you to work with other people on the same project, while guaranteeing that you won't mess up each other's work.
 * Git allows you to send and receive files from your computer to a server, making it easy to put your projects online.

** Concepts
*Commits:* Think of a commit as taking a snapshot of your project. Every time you make a commit, you are saving a record of what your project looks like at that moment in time. This allows you to track changes over time and gives you a perfect history of your entire project.

*Repositories:* Where your project lives. Itâ€™s like a folder that stores all of your code and the entire history of your project. Repositories can live on your local machine or be hosted online via platforms like GitHub.

*Staging Area:* The staging area is like a holding ground where you prepare changes before making a commit. Think of it as packing a suitcase for a trip. When you lay everything out on your bed, thinking about what you want to bring with you, you can still move around, remove, and add items. The bed would be the staging area, and "committing" would be closing and locking your suitcase.
** How GitHub Works
GitHub stores your Git repository online. While Git is entirely local (everything is stored on your computer), GitHub hosts your repository online so that you can share it and collaborate with other people.
** Branches and Pull Requests
In Git, branches allow you to work on different versions of your project simultaneously. For example, if you are working on a major feature but you donâ€™t want to disrupt the rest of the project, a branch would allow you to create an alternate timeline where you can make changes without affecting the main project.

*Pull Requests* are used when working with other people to propose changes. If someone has code that they want to contribute to the project, they create a pull request to merge their work. This allows other team members to review the code before itâ€™s integrated.

** Glossary
Core Git Terms:
- Repository (Repo): A projectâ€™s directory or storage space where your projectâ€™s files and their revision history are stored.
- Commit: A snapshot of your project at a specific point in time. Each commit has a message and stores changes made to the code.
- Branch: A separate line of development within the same repository. You can think of branches as different versions of your project that diverge from the main version (typically called main or master).
- Clone: The process of creating a local copy of a repository from a remote server (such as GitHub).
- Fork: A personal copy of someone elseâ€™s repository, typically used to suggest changes or to start independent development.
- Pull Request (PR): A GitHub-specific feature. It allows developers to request that their code changes be merged into the main repository. It often involves code review before merging.
- Merge: The process of combining changes from different branches into one. It typically happens after a pull request is approved.
- Rebase: Similar to merging but instead of combining the changes from different branches, it rewrites the commit history by applying your changes on top of another branch. Useful for keeping a cleaner commit history.
- Remote: A version of your project hosted on a remote server (such as GitHub, GitLab, or Bitbucket). The remote is usually referred to as origin.
- Fetch: Retrieves the latest changes from the remote repository without applying them to your local repository.
- Pull: Retrieves the latest changes from the remote repository and applies them to your current branch.
- Push: Sends your committed changes to a remote repository (e.g., GitHub) so others can access them.
- HEAD: The current snapshot or commit that your working directory is based on. Itâ€™s typically the latest commit on the active branch.
- Staging Area: A place where you prepare changes before committing them. Think of it as a holding area where you decide what will go into your next commit.
- Index: Another name for the staging area.
- Checkout: The process of moving between different branches or commits in a repository. You are switching your working directory to match a particular branch or commit.
- Conflict: Occurs when Git is unable to automatically resolve differences between two commits (such as during a merge) because the same line of code was changed differently in both branches.
- Cherry-pick: Selectively applying changes from one commit in another branch without merging the entire branch.
* DONE Analyzing soil with a XRF
CLOSED: [2024-10-21 Mon 08:56]
:PROPERTIES:
:EXPORT_FILE_NAME: analyzing-soil-with-xrf
:END:

/Note: This article is a draft and will be modified./

*Warning*: The XRF emits X-rays up to 50kV. Improper use can and will cause serious, long term harm to your health. Follow all safety protocols including those beyond this article.
** Introduction
This article assumes you already are in possession of pucks of finely ground soil.
** Procedure
1. *Prepare the XRF Analyzer*
   - Take the battery from the charger and insert it into the back of the XRF.
     - Note that a good amount of force is required to latch the battery into place.
   - Power on the XRF by holding the power button on the left of the front for about 3 seconds.
   - Wait for the system to boot.
   - Log into the device using the password.
   - The four directional buttons are used to select items on the interface.
   - The button to the right of the pad is the enter or confirm button, not a back button.
   - The center of the pad is not a button, which is incredibly unintuitive and confused me for an unbelievably long period of time.
   - From the main menu, select =System Check= to calibrate the scanner.

2. *Connect the XRF to a Computer*
   - Open the =NDTr= software from the start menu on the desktop. It should automatically connect to the XRF and mirror the deviceâ€™s screen on the computer.
     - If it doesn't, try some of the following.
     - The software should be the /only/ XRF-related piece of software open. If something else is open trying to communicate with the scanner, it won't work.
     - The scanner should be communicating on COM6. To ensure this is the case or to change it:
       - Maximize the window with the window decoration controls (e.g. _ â–¡ â˜’)
       - Click the now visible settings button.
       - Change the COM port to 6.
       - If it still won't work, ensure that the XRF is actually communicating on COM6.
         - Open Device Manager (search in start menu), and under "Universal Serial Bus", look for the scanner. Note the COM port.

3. *Prepare and Name the Samples*
   - Slide the latch on the XRF stand to the right and open the lid.
   - Place your sample puck on the XRF with the film side facing down.
   - Close the lid and lock it by sliding the latch back to the right.
   - Navigate to the =Sample Type= menu and select =Soils and Minerals=, then choose =Soils=.
   - Navigate to =Data Entry=, and click the glyph of the keyboard to enter the sample's name.
     - Note that you can use the computer's keyboard to type.

4. *Perform the Scan*
   - Ensure that the sample is centered on the sensor and that the lid is locked, then press and hold the trigger. The XRF will scan as long as you are holding the trigger, up to 10 seconds.
     - Ensure you hold the trigger for the same amount of time for all samples in a batch.
     - The longer the scan, the more accurate the results are, although scanning for more than 5 seconds is essentially redundant.
   - After the scan completes, open the lid and remove the sample, placing it on the lab bench film-side up.
     - Note: placing the sample film side down can easily destroy the puck.
   - Press the return key on the computer keyboard.
   - Enter a new sample name, and continue with the next sample.

5. *Download the Data from the XRF*
   - Once all samples have been scanned, click the =Disconnect= button on the computer and close the =NDTr= software.
   - Open the =NDT= software. The XRF should be connected to COM6.
   - Click =Test= to verify the connection. If successful, click =Query Readings= to retrieve the data from the XRF.
   - Scroll to the bottom of the readings list, select the samples you need, name the file, and click =Download= to save the data.
   - .ndt files can be opened in Excel or your text editor of choice.
* DONE Creating animations of soil cores out of CT scans
CLOSED: [2024-11-20 Wed 13:53]
:PROPERTIES:
:EXPORT_FILE_NAME:  creating-animations-of-soil-cores
:END:
** Introduction
Goal: To create animations of soil cores spinning from .vol files obtained from the Pacific Northwest National Laboratory.

We're getting our files from the Environmental Molecular Sciences Laboratory at the Pacific Northwest National Laboratory, who have scanned the soil cores with a Nikon XTH CT scanner.
** Requirements
- .vol files from a CT scanner
- At least 16gb of memory
  - [[https://downloadmoreram.com/][Ideally more]]. Both Fiji and Dragonfly attempt to load the entire file into memory; the downloaded .vol is almost 40gb.
- [[https://dragonfly.comet.tech/][Dragonfly from Comet Software]]. You'll need a license: [[https://www.theobjects.com/dragonfly/get-trial-version-request-comet.php][30 day free trials]] are easily accessible, and [[https://dragonfly.comet.tech/en/non-commercial-licensing][non commercial licenses are available]].
- [[https://fiji.sc/][Fiji]] or ImageJ, but use Fiji.
- A windows or Linux machine to run Dragonfly on; everything but that can be done on Mac
** TL;DR
- Download .vol files from [[https://sc-data.emsl.pnnl.gov/#state=32f73ab7-1755-4bad-8f02-a5640adf3a1a&session_state=0f3eb580-fa1b-48b2-aed4-6eefcad788f9&code=6f48e754-f383-442b-884b-f7222be36fbe.0f3eb580-fa1b-48b2-aed4-6eefcad788f9.21cf84f6-6ada-4d20-8b12-72f0a3e0bce3][EMSL Data portal]]
- Import the files into Fiji as raw data, 2000x2000x2000
  - Convert to 8-bit, save as .tiff
- Open the .tiff in Dragonfly, crop to slices 250-1800
  - Make a cylindrical mask to crop the garbage off the core
  - Use movie maker to make a rotating movie, export to .avi
- Use Handbrake or similar to convert to mp4
- Use Final Cut Pro or similar to crop to square
- Use Gifski to convert to reasonably sized gif

** Getting .vol files from EMSL data portal
*** Selecting cores
To start, we need to get the CT scan files from EMSL, provided as .vol files.
- Navigate to the [[https://sc-data.emsl.pnnl.gov/#state=32f73ab7-1755-4bad-8f02-a5640adf3a1a&session_state=0f3eb580-fa1b-48b2-aed4-6eefcad788f9&code=6f48e754-f383-442b-884b-f7222be36fbe.0f3eb580-fa1b-48b2-aed4-6eefcad788f9.21cf84f6-6ada-4d20-8b12-72f0a3e0bce3][EMSL data portal]], and login.
- On the left hand side of the screen, open the "Project" drawer and scroll down to the "Principle Investigator" field.
- Enter the first few characters of a name, and then check the cooresponding box.
- The main panel should update with the relevant project.
- Scroll down to the "Select Datasets" button, and click it.

You should now see a list of the datasets associated with that project. In this example, we're looking for individually numbered soil cores, but you will note that none of the data are labeled.

If you have a specific core number in mind, the only way to locate it is by looking inside of every single dataset by clicking the "DOI" button next to the Upload ID. You can then see the individual file names, which include the soil core number. If you decide to embark on a search to find a soil core, I recommend sorting by Instrument to only see XCT data, and then sort by file size. You are looking for datasets that are 29.8GB. Only view 10 datasets per page, as when you exit the DOI file viewer the entire webpage snaps to the top; it's easier to keep track of where you are when there are only 10 items.

Once you find the core you want, note the Upload ID. To search for an ID you already know, set Items per page to All, and use cmd + f.

Some known core ID's:
| Core # |  Bottom |     Top |
|      1 | 3132984 |         |
|      2 | 3134562 |         |
|      5 | 3135086 |         |
|      6 | 3135290 |         |
|      7 | 3049610 |         |
|     10 | 3133330 |         |
|     11 | 3051409 | 3051495 |
|     15 | 3010819 | 3010713 |
|     16 | 3011663 | 3013268 |
|     20 | 3010658 | 3010660 |

Some notes:
- Each core is 30cm tall, and the bottom 10cm and top 10cm were scanned.
- Core height
- 2000 slices, how much is each slice
- Some cores (Listed above: 1 & 6) are difficult to use because they were sampled on a hill or something and its like just dirt and air

*** Downloading cores
Add each dataset to your cart by checking the box next to it. To download the cores, you will need to set up and use Globus I realllly do not want to detail how to do that

Once EMSL prepares the files, you will see a link to go to the created guest collection. Open it, and navigate to your endpoint (computer) on the pane without the .tar in it. Go to the folder where you want the files to be downloaded to. Once there, select the tar on the other pane, hit "Transfer or Sync to..." in the middle, and then click start above the pane.

Globus will start transferring the file. You should get an email when its done. For reference, a 96GB transfer took a little over three and a half hours.


** Initial .vol work in Fiji
Unzip the tarball, and navigate through the mess of folders inside to get to the .vol files (each should be around 30GB). I recommend moving the .vols into their own folder, separate from the tarball's filesystem, because it's obnoxious to have to go 30 directories deep to get to them.

Notes:
- Each .vol has a corresponding .vgi that contains metadata about the .vol. Open it in a text editor if you want to know more.
- There are a number of other derived products in the tar, namely still images of the core and csv files with data (notably porosity).

Each .vol contains 2000 images, each of which is 2000x2000 pixels, totalling *8 billion voxels*, each of which is represented as a 32-bit floating-point number and saved in an uncompressed file. This makes the raw files wildly impractical to work with, hence the use of Fiji. The goal is to convert each file to 8-bit and save it as a .tiff, both of which should drastically reduce the size while losing no useable data.

Open Fiji, and use File > Import > Raw... to open the .vol.
- Image type: 32-bit Real
- Width: 2000
- Height: 2000
- Offset: 0
- Number of images: 2000
- Gap: 0

Check Little-endian byte order and Use virtual stack. Virtual stacks load the images without fully committing them to memory, making it possible to view datasets that are larger in size than the amount of memory you have. *If you don't have at least 40GB of memory and don't use virtual stacks, Fiji will crash your system attempting to load the file.*

Fiji will open the file, and you should be able to scroll through the slices of the core, or use the scroll bar at the bottom of the window containing the core.

To convert the file to 8-bit, use Image > Type > 8-bit. This will take time: you should be able to see Fiji converting images by the hundreds on the main toolbar.

You should now have an updated view of the core; notice the reduced file size displayed in the window title. To save as a .tiff, use File > Save as > Tiff...


** Working in Dragonfly
*** Loading & masking
In Dragonfly, open the newly created .tiff file with File > Import Image Files... . Click the Add... button, then select the file. Click Next in the bottom right.

The fields on this screen should be correct. Ensure that the Total size (on right, under Information) is less than the amount of memory you have.

Click Crop Image... and preview the image of the soil core. If the core doesn't take up most of the frame, crop it by dragging the green dotted edges or corners inward.

In the Crop Image... window, crop the image to slices 250-1800. Under the Cropping slider, enter 250 in the first field and 1800 in the second. Click OK.

Click Finish. After loading, you should see four views of the core: a 3D render and three orthogonal views. You can maximize a pane by double clicking, and you can move the 3D view by clicking and dragging in its pane. Double click again to return to all four views.

Notice the "fuzziness" or garbage data visible in the 3D view (most visible looking top down, around the edges). To remove this data, we need to create a cylindrical mask.

At the top of the 3D view, there is a row of buttons under the Shapes bar. Click the cylinder button, the third from the left - note the capsule button right next to the cylinder button.

You should see a wireframe cylinder insdie of the soil core. Using one of the side orthogonal views, drag the green lines outward, and stop when you hit the garbage data.

You should see the cylinder on the other views increasing and get a sense of what you're doing. Everything outside the cylinder will be excluded.

On the right hand side of the screen, you should see a list of the objects loaded: the core and the cylinder. Click on the cylinder, and look under Shape properties, Visual effects, Visuals. Check the box next to the core. Check "Outside", then under 3D effects, "Clip". You should see the 3D view update to exclude anything outside the cylinder.

*** Animating
Right click the 3D view and select Show Movie Maker. Double click the 3D view pane to maximize it. You'll need to position the 3D view to be what you want the video to look like: I position the core straight on, with the camera slightly raised above it at an angle so the top is visible.

Click and drag to manipulate the core, and click and hold the middle mouse button and move your cursor up and down to zoom in and out. To physically move the core in the 3D space, press x on your keyboard and click and drag. Make sure to press escape when you are done moving it to return to the default track tool.

Once you've positioned the core, look at the timeline near the bottom of the screen. At the 00:00 mark, you should see a square with a picture of the core in it. Right click it, and select "Update key frame".

To actually rotate the core, click the "Rotate" button above the timeline. Choose one of the two right-most options (I use clockwise), and select "Rotate around the object's axis". A second image, or keyframe, should appear on the timeline. Drag this second keyframe out to around the 20 second mark, or however long you want the video to be.

The original keyframe is where the animation will start, and the ending keyframe is exactly the same except that the core has been rotated 360 degrees. Dragonfly interpolates the motion in between the keyframes, so by dragging the 2nd keyframe further out along the timeline you are causing the core to spin slower.

You can drag the green bar marker along the timeline to preview the animation, or click the play symbol above the timeline. Note that the animation likely will not play at full speed, as it's rendering in real time.

To change the color of the background to a solid white, click the "Scene's Views Properties" bar in the left list of toolbars. Near-ish the top is a button next to "Background color". In the window, you'll want to select "Uniform" under mode, and then click the button next to "Color 1" in the Color section on the right. In the color picker, drag the right mode slider up from black to white, and click OK, then OK. You should see the updated background.

To export the animation, click the "Export animation" symbol above the timeline, immediately next to the start / beginning symbol. Set the FPS to 60, the Dimensions preset to 1920x1080, and the bitrate to High. Click Save as Video File. Once you've given a name and location for the file, Dragonfly will render and export the animation as an .avi file.

** Working with .avi video files.






** Ideas maybe
- Make little gif animations



** GIFS:
- GLOBUS transfer
- fiji import dialogue
- fiji converting to 8-bit
- dragonfly import
- dragonfly cropping
- adjusting cylinder
- outside, visual effects, clip
- dragging keyframe
- export animation from dragonfly
-


- Screen recordings of dragonfly might be much easier to digest than explaining the ii over text
-
** Soil cores
- Cores from EMSL data portal
  - 11, 15 AND 16, 20. Bottom if needed to choose
    - 15 bot: 3010819 top: 3010713
    - 16 bot: 3011663 top: 3013268
    - 11 bot: 3051409 top: 3051495
    - 20 bot: 3010658 top: 3010660
    - 1 bot: 3132984
      - Had issues: use 2 bot: 3134562
    - 5 bot: 3135086
    - 6 bot: 3135290
      - Had issues that might make me use 7 bot: 3049610
    - 10 bot: 3133330

      Made gifs for 11, 15, 16, 20.
      Need to make gifs for: 2, 5, 7, 10

  - 11/8 update downloaded 2, 7. Now making gifs of those and finalizing gifs of others
* TODO About
:PROPERTIES:
:EXPORT_FILE_NAME: Creating-this-website
:END:
** Writing and formatting
All posts are written in Emacs 29.4 in Org mode.
** Publishing
Posts are published to markdown using ox-hugo, and pushed to GitHub using Magit. Hugo is used as a framework for the website, with a modified version of the papermod theme. Cloudflare pages is used for building the site.

Written in Emacs 29.4, using Doom 2.0.9, in Org mode 9.7. Published to markdown using ox-hugo, and pushed to GitHub using Magit. Hugo is used as the website framework with a modified version of the papermod theme. Cloudflare is used for domain management, and Cloudflare pages builds the site.

